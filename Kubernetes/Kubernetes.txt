 	KUBERNETES

IN THE MASTER - 
	1) API SERVER - is the reception of the 
	2) ETCD - is the database of the kubernetes cluster 
	( ETCD contains all the data of the pods, users or ny data)
	3) CONTROLLER MANAGER - it is going to control the worker nodes 
	(checks if the worker node is running or not, what  is the capacity of the worker nodes, etc)
	4) SCHEDULER - creates the pods and the containers 
	
all of the 4 are going to run as containers on the master server

we can send messages to the kubernetes master through -  
1) UI - user interface
2) CLI - Kubectl (cli tool for kubernetes is kubectl)
3) API - Application programming interface

any of the commands through the above methods will reach the api server of the master first : 
Api server is like the reception area 

kubeadm - command to add node, manage node or anything with the node

kubeadm init (on master)

kubeadm join (on the worker)

Any instructions from the controller manager is going to connect to the worker nodes 
scheduler manager will connect to kubelet - hence agent based communication

STEPS : 

name all the nodes as master and worker nodes
(in real time we have the names as master.example.com - example would be the company name)


Docker configuration on master and worker nodes : 

 cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
"max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF


sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo swapoff -a

sudo kubeadm init

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
cat ~/.kube/config (shows the info of config file)

Install Container Network Interface (CNI)
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml

Worker Nodes initialization - Worker1 & 2

sudo kubeadm token create --print-join-command
(Add sudo to the join command inside the worker nodes)

Verification:

kubectl get nodes (on the master node to see the nodes)

sudo kubeadm reset -f (resets the node) [do not do it unless necessary]

free -g (shows the memory in gb)
or 
free -m (shows the memory in mb)

sudo kubeadm init (initializes the master as administrator)

If you want to run the kubectl commands on the worker nodes then copy the admin.conf file to the worker 
nodes... copying the admin.conf file so that the kubectl can connect to the api server

sudo kubeadm token create --print-join-command


kubectl get nodes -o wide (shows extra columns of the node info in a list)

kubectl describe node <nodename> (shows detailed info about the nodes)

kubectl config view (shows the config file information with all the contexts)


CONTEXT IS THE ENVIRONMENT WE ARE IN (i.e., dev, staging or prod)

kubectl config (shows the modules) 

kubectl config current-context (shows the current context we are in)

kubectl config use-context <contextname> (switch to other contexts)

ex: kubectl config use-context kubernetes-admin@kubernetes

kubectl config get-contexts <contextname> 

kubectl cluster-info 

kubectl cluster-info dump > /tmp/cluster-dump (creates a cluster-dump folder with the cluster dump 
output)

if we have 4 diff app in our company what we will do is we create a namespace inside the vms 
where in each namespace a specific app container only is present

app1 namespace -- app1 containers only
app2 namespace -- app2 containers only

if we have 2 worker nodes -- we will logically create a namespace for each and every application
so if we have about 40 gb memory and we have 4 apps we will allocate the memory to each of the app
as much as the app requires 
if app1 need 15 gb -we allocate 15gb and app2 needs 5 gb we allocate 5gb and vice versa so that all 
the applications adjust in the worker nodes

HOW TO CREATE NAMESPACES
kubectl get pods -n <namespace-name> 
kubectl get all -n <namespace-name> 

kubectl get pods -n kube-system (shows the container pods with the specific namespaces kube-system) 
{add -o wide for more info}

-n is used to create namespaces

kubectl get pods -A (shows all the pods irrespective of namespaces)


REGISTERING WORKER NODES : 

to join the worker nodes to the cluster - 
1st method we did was using join command 
2nd method is - 

create a .json file

kubectl apply -f <filename> (to execute the file)

kubectl get nodes (to see the nodes)

to create a pod : 
kubectl run <podname> --image <imagename> --port <portnumber> (to create a pod)
kubectl run nginxpod --image

kubectl get pods (shows pods)
kubectl exec -it <podname> /bin/sh (to enter into the pod)
kubectl exec -it <podname> <linuxcommand> (to run any one command on the pod)

kubectl logs <podname> (shows logs)

KUBERNETES DASHBOARD : (UI)

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/... (creates kubernetes dashboard)
(to get this command search kubernetes dashboard in google and download the yaml file from there)

kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/... (deletes the kubernetes dashboard created)

kubernetes service means the loadbalancer 

kubectl edit svc -n <namespace-name> <namespace-name>
kubernetes get all -n kubernetes-dashboard 

open browser in the master node machine on the desktop 
htts://localhost:32012
token - 
kubectl -n <namespace> describe secret 
Devops engineers wont use UI -- we use it to show others to see the pods like agile scrum master,etc

BACKUP : 
the first thing as we see the database is to take a backup 
the main thing we do for a database is backup

how to take a backup for ETCD : 

etcdctl (suggestion to install etcdctl)
sudo snap install etcd
sudo apt install etcd-client

to take backup - we need to connect to etcd through username and password

Controller manager connects to the etcd through the pki file in /etc/kubernetes which contains all 
the certiicates
etcdctl snapshot save (you will get the whole command from the kubernetes.io website)

etcdctl snapshot status (shows the information of the backup snapshot which was taken)

restoring only works on a new etcd or if we have a brand new cluster... it wont work on the same 
cluster
Restore is only done when we have a multi master setup
etcd-backup.db (file in which the backup is stored)

UPGRADING KUBERNETES : 
kubeadm version
kubectl version
kubelet --version
kubectl get nodes
These all show the versions of kubernetes

kubeadm upgrade 

WORKLOADS : 

Main containers and side car containers - 
main - app, side car containers - security container or storage

we create a minimum of 1 and maximum of 5 containers per pod

Purpose or advantages of having multiple containers in a pod - 
- latency is reduced between containers 
- storage can be common for all the containers - 
(same shared volume to have the same shared volume so that app and logs container can use the same shared volume)
- IP address can be shared for all the containers - 
(they have the same ip address as pods have the ip address) 

the 3 major components of a VM or container
Compute - Cpu and memory 
network - 
storage - 

the sizes of the containers will vary inside a pod - example : app container will have more size than 
other containers
Note - kubernetes manages only pods and not containers
we cannot also add or delete containers to a running pod

HOW TO CREATE A MANIFESTS FILE : 

Manifests file contains : 
apiVersion: v1
kind: Pod
metadata:
  name: imagename
spec: 
  containers: 
    - name: 
kubectl api-resources ()

kubectl explain pod (shows the api version, kind, metadata, specs - how to create a manifests file)
kubectl explain pod.apiVersion (shows to create manifests file apiVersion)
kubectl explain pod.spec.containers (shows containers)
kubectl explain pod.spec.containers (shows mages)
kubectl explain pod.spec.containers --recursive 
(shows everything related to creating the manifests file)

cat pod.yaml (is the manifests file)

kubectl apply -f <filename> (to create pod from manifests file)
Note : pod status will be running only if the contaienrs are in a running state
podname and container name can be different 

INIT-CONTAINER : 

init container will start first and if the init container is successful only then the main container 
will start 

kubectl get pods <podname> -w (-w is watch state, the output keeps changing when a new event in 
the command occurs)

RESOURCE LIMITS : 

Assigning limits to a container - like cpu ram, memory, etc

STATIC PODS :

static pod - when a pod is run only on one node. K8s admin chooses which worker node to run the pod
creating a pod from the worker node through kubelet
USECASE : we can monitor the traffic or any suspicious activity by creating a pod on the worker node 
and it cannot be controlled by master node if we want 

ADDING A .yaml manifests file inside the /etc/kubernetes/manifests creates a static pod inside the 
node
How to identify static pod from normal pod? - static pod will have the node-name attached to the 
podname
so any pod with the nodename attached to it is a static pod

PET VS CATTLE : 
the way we manage pet is different than how we manage cattle
return from cattle is much more than pet
pod = pet , deployment= cattle

Deployment will delete the existing pods and create new pods

kubectl get deploy (shows all deployments)

kubectl explain deploy (explains how to write manifests deployment file)
kubectl explain deploy.spec
kubectl explain deploy.spec.template.spec

kubectl get pods
kubectl get replicaset 

kubectl get deploy (shows the deployment)
deployment - is simillar to service in docker... it contains replica pods with same containers, only 
the name is different

ADVANTAGE OF DEPLOYMENT: 
1) we can create same multiple pods
2) whenever there is an update, we can apply it through deployments
3) scaling the number of pods - increase or decrease

kubectl rollout history deploy <deploymentname> (shows all the history of the deployment)
(Revision is same as commit in git)

UPDATING: 

kubectl set image deployment/<deploymentname> <containername=imagename> (updates the present image)
kubectl rollout history deploy <deploymentname> (now it shows the second revision... it indicates it has updated)
kubectl rollout history deploy <deploymentname> --revision=<no.of.your-revision> (describes the revision)
kubectl get pods (new pods and containers are created with the updated image)
kubectl get replicaset (shows the different replicasets)
kubectl get rs (shows the different replicasets)
for each and every revision a replicaset will be created (revision is same as commit in git)
rolling update - in case of update kubernetes will create one pod and test it if it running good and 
then it will delete the previous pods and create new pods
CANARY DEPLOYMENT(in devops) - same as - ROLLING UPDATE(in kubernetes)
rolling update - in case of an update if one pod is successfully created then the previous pod will 
be deleted and then other pod is created another pod from the deployment is deleted and so on

kubectl set image deployment/<deploymentname> <containername=imagename> --record (record will set a 
change cause or message as to why the revisison had been done- simillar to commit message in git)

Check the change cause by : kubectl rollout history deploy <deploymentname> 

ROLLBACK: 

kubectl rollout undo (deployment goes to previous version of the image)

kubectl rollout undo deploy <deployment-name> (rollback to the previous version)
kubectl get pods | grep deploy 

kubectl rollout undo deploy <deployment-name> --to-revision=<no.of-revision-u-want-to-rollout-to>
(to go back to any revision number you want)
kubectl rollout undo deploy nginx-deployment --to-revision=1 (to rollback to revision 1)

MANUAL SCALING: 

scale up and scale down 
kubectl scale --help (shows all the modules)
kubectl scale deployment <deploymentname> --replicas=<no.ofreplicasuwant>
kubectl get rs or kubectl get replicaset (shows the replicasets)
kubectl get deploy (shows the deployments, depending on the number of rs u can scale up and down)

when to use replica mode and glocal mode: 
replica mode is to scale up and down, glocal mode cannot be scaled
global mode is only used when we need only one container in all the nodes

DAEMON SETS: 
there will be one pod which will be running on all the nodes 
Realtime use case: fluentd
Fluentd - it will collect the logs from any application and will store them in a central space

kubectl get daemonset or kubectl get ds -A (shows all daemon sets) 
(added -A for all namespaces) 

daemonset.yaml

kubectl apply -f daemonset.yaml

kubectl get ds -n kube-system 
kubectl get pods -n kube-system -o wide | grep fluent

JOBS AND CRONJOBS: 

updating the database : 
the app cam be updated by set image but how to update database? 
adding the tables and then upadting the deployment: 
On the existing database pods we need to add tables 

A migration job pod is created (simillar to init container) and when it is successful a new pod with 
version 2 is created 

Tool to update : ArgoCD 

ArgoCD: 
step -1 : add 8 new tables [job]
step-2 uber 1.5 [update deployment]

Job pod: 
a given job is done by the created pod and it remains without using any further resources 
suppose the job is to print date or message it creates a pod and prints the message and remains
k logs <podname> 
crontab - for scheduling the job
***** 
1st* - minute 
2nd* - hour 
3rd* - day of the month 
4th* - month 
5th* - day of the week

kubectl apply -f cronjob
kubectl get cronjob
kubectlget jobs
kubectlget jobs -w ( -w to see the execution status of the command)
New jobs/jobpods get created everytime based on the schedule set in the cronjobs

Configuration Basics : 

in the app we have 2 files :
1) code --> container image
2) config --> 
Code and config have to be kept seperately 
Config should be kept different for each and every environment - like diff env like dev, staging and 
prod - different usernames for web in dev, staging and prod and also diff passwords for diff env
different usernames for app in dev, staging and prod and also diff passwords for diff env
different usernames for DB in dev, staging and prod and also diff passwords for diff env

For creating microservices: 
Standard for creating microservices - configuration is changing between environments
configuration will be created as a property file in dev, staging and production seperately 
config file includes the database name, username and passwords 

ENV : 

(env in the command line will show various environment variables)
echo $USER - shows current user

cat env 
k apply -f env 
k exec -it envar-demo /bin/shv
inside the pod :
env - shows all the environment variables

CONFIG MAP : 

is the configurations which are being stored in the kubernetes
advantage of config map over env - if there a multiple deployments, they can access frm a central 
location

cat cm
kubectl aply -f cm 
k get cm
k describe cm <cm-name>

we should not store passwords in a config map as it is human readable and not encrypted

if we want to apply the changes to the config map  - we have to delete the pods and then create new pods 
- suppose if we want to change the username or password of the env 
k rollout restart deploy <deployment-name> (all the previous pods get deleted and new pods will be created)

k get pods show the newly created pods in the deployment 

(Note: it is not recomment=ded to change anything in the running pod, rollout or deleting and creating
new pods is proper method) 

whenver an application is getting deployed it will look for the config map file
the file will be stored in the volume 

(Note: busybox- is one of the smallest container images used for testing the pods)

SECRETS : 

we store all the passwords inside the secrets(k8s) - the password will be encrypted
if we store both the username and password inside the secrets - bt will be encrypted
when we store inside a pod it will be decrypted
DevOps-> usr/pwd-> secrets(k8s) - Encrypted
Deploy -> Pod -> Container -> Decrypted

cat sec 
k apply -f secret 
k descrice secret <secretname>
if we creare a pod with specifying secret the it will not be in an encrypted format 

Note: if it is a non-sensitive data we can store the variable in the config map and if it is a 
sensitive data we can store it in the secrets

LABELS AND SELECTOR : 
We can set labels to all the kubernetes servcies 
kubectl get nodes --show-labels (shows all the labels of all nodes)
kubectl get nodes -l kubernetes.io/os=linux (shows all the nodes with linux os)
use case scenario - when we want to specify the linux nodes only or any other resources only we can 
apply the label search
based on the labels we can select onto which node we can deploy the pods 

Selector : where we mention which label to choose to deploy a kubernetes service

2 tier architeccture (example): 
Frontend deployment(3 nodes) --> Redis master deployment (1 node)
			    Redis slave deployment (2 nodes)

kubectl apply -f kubesample.yaml
k get pods -l tier=backend (shows all the pods in which the label tier=backend was set)
k get pods -l tier=frontend (shows all the pods in which the label tier=frontend was set)
(label is simillar to grep function)

to query 2 or more different labels:
kubectl get pods -l 'tier in (backend, frontend)' (shows all the backend and frontend pods)
kubectl get pods -l 'tier in (backend), role notin (slave)' (shows all the backend pods and shows the 
master pod and hides the slave pods)
kubectl get pods -l 'tier in (backend), role in (master)' (shows all the backend pods and shows the 
master pod)
(in - shows the given label notin - hides the given label)
kubectl get pods -l 'tier in (backend), role in (slave)' (shows all the backend pods and shows the 
slave pods)

MONITORING AND LOGGING : 
(logging - fluentd, metric server - tools with which we get the usage of memory and cpu,etc)
kubectl top pods (shows cpu and memory of all pods) (inbuilt monitoring tool)
(other montioring tools - prometheus, grafana, etc)
kubectl top nodes (shows cpu and memory of nodes)

AUTOSCALING :
adding or removing the nodes based on the resource utilization

Cloud VM AutoScaling : (autoscaling for nodes)
Desired:3 Max:5
Threshold
  CPU -min:10% max:80%
explanation: scaling up - if the load on all the nodes is more than 80% then the 4th node will be created and if 
the load is increased for more than 80% on the 4th node too then the 5th node is created 
if the 5th node is also getting a load more than 80% then it will create an alert to the devops 
engineer to increase the nodes by validating the issue or because of the user number increase 

scaling up - if the load is lesser than 10% of utilization then the 4th or the 5th node will be deleted

Kuberenetes pod autoscaling : (autoscaling for pods)
replica:3 max:5
threshold
  CPU min:10% max:80%
explanation: simillar to nodes

HORIZONTAL POD AUTOSCALING (HPA): 
kubectl autoscale --help
kubectl scale deploy <deployment-name> --replicas=2 (setting the replica number to 2)
kubectl autoscale deploy <deployment name> --min=3 max=5 --cpu-percent=40 ( autoscaling)
kubectl get deploy shows min 3 pods created
kubectl get hpa
kubectl autoscale deploy nginx-deployment --min=4 --max=5 --cpu-percent=40
kubectl get pods -l app=nginx
kubectl get hpa

SELF HEALING PODS : 
probes - 
probing - testing if an app is running or not in regular time intervals

2 types of probes: 
1) Liveness probe 
detects when a pod enters
2) Readiness probes

Pod 
  ContainerA 
  ContainerB (app container)
    Probe: Liveness (container) and Readiness (app)
       URL: api/health.html 
           period: 5 sec 
  ContainerC 
It keeps checking for the file for every 5 secs 
(if the response comes from this example file then the container is running or else there is 
some issue)

Liveness - whether ur container is running or not 
Readiness - whether ur application is running or not

liveness - when the probe fails then the container automatically goes into restart as such
A temporary file is created and a cat command is passed to check if the file is present and we 
are getting a response of cat from that file then the probe passes

readiness - when the probe fails then the container is stopped 
Readiness looks for application status
A temporary file is created and a cat command is passed to check if the file is present and 
we are getting a response of cat from that file then the probe passes
- only when the container is running it will accept the traffic

Liveness checks for the container and readiness checks for the application
if liveness probe fails then it will restart the container 
if readiness probe fails then it will stop the traffic and the container is stopped 

kubectl explain pod.spec.containers.livenessProbe.failureThreshold
kubectl explain (explains any of the specs u want - very useful to know things)

SCHEDULING :

node selector - 
nodeName - to be used to specify which node the pod has to be created
nodeSelector - 
using node labels : 
Note: How to dynamically add labels when the resources are running -
kubectl label node <nodename> <labelname=value>
kubectl label node workernode1 disktype=ssd (setting a label disktype=ssd)
kubectl label node <nodename> <labelname-> (to delete a sepcific label)
kubectl label node workernode1 disktype-  (setting to delete a label disktype)

NODE AFFINITY : 

Affinity - means if condition is TRUE --> pod gets created 
  condition (node selectors) --> node affinity
  condition (pod selectors) --> pod affinity
Anti-Affinity
  if condition is TRUE --> pod will not get created   
  condition (pod selectors) --> pod Anti-Affinity

Condition
  requiredDuringScheduling = Must have
  preferredDuringScheduling = May have
kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution (gives description)

kubectl label node <ndoename> disktype=ssd (to set disktype label)
label node <ndoename> disktype=ssd network=fast (to set disktype and network label for the nodes)
to delete the labels: 
kubectl label node <nodename> <labelname>-
ex: kubectl label node <nodename> disktype-

kubectl get nodes -l disktype=ssd (to see disktype label)
kubectl get nodes -l disktype=ssd network=fast (to see disktype and network label for the nodes)

POD AFFINITY : 

Redis 
  - 2 pods on the same node
    if condition is TRUE - do not create a pod 
    Anti-Affinity
Webserver - 2 conditions
  - 2 pods on the same node
    if condition is TRUE - do not create a pod 
    Anti-Affinity
  - only create when redis is there
    Affinity

Note : first we deploy the backend, i.e., redis and then the forntend, i.e., webserver

TAINTS AND TOLERATIONS : 

Taint is like a label set on nodes 

taint is a property which is set on nodes and tolerations is a property which is set on pods
if toleration is set to the taint property then the pod will be created on the node

for a specific app which requires specific dedicated nodes for themselves- if taints are set for the 
nodes, then if we specify the tolerations then only the pods will be created on the nodes, any 
manifests file with default values does not create any pods on these nodes 

SECURITY CONTEXT : 

if there is any application which needs elevated privileges as an admin or root then they go with 
security context by providing user id and group id

PRIORITY : 
if there is a pod priority being set then the pods which are set as high priority they will be 
created first

kubectl get Priority Class (shows all the PriorityClass with the default PriorityClasses)

Note : kubectl api-resources (to know the api versions of all the manifest files for different services 
like deployment, pods, nodes, configmaps)

NETWORKING : 

Note: Deployment - means identical pods

loadbalancer - when the traffic comes from the users, the request first goes to the loadbalancer and then the 
loadbalancer sends the traffic to the available pods 
loadbalancer is a service which receives the requests
loadbalancer is referred to as service 

All the different kinds of traffic will be received by a service port  --> and all the traffic which 
is received on the service port will be sent to the target port --> and the target port will divert 
the traffic to any one of the container port
target port is exactly simillar to the contianer port... ex: if the port is set 80 for a deployment 
then all the containers will have the same port as 80 and the target port as well will be 80
- Service port can be anything 

Types: 
Network loadbalancer
internal loadbalancer 
external loadbalancer 
application loadbalancer

We have a service for each deployment

PODS ARE EPHEMERAL - ephemeral means they are temporary,i.e, when they are updated the old pods are 
deleted and new pods are created

SERVICE DISCOVERY : 

service needs to identify all its pods, so it will use same labels to query and it will get the pods 
which have the same labels
whenever new pods are created it will look for the labels

kubectl apply -f kube-sample.yaml
kubectl get svc <servicename> (shows service details)
kubectl get endpoints <servicename> (shows the endpoints or ips the service can connect to)
kubectl get ep <servicename>
kubectl describe svc <sevricename> (shows all the details of the service with the service port and 
target port and the endpoints, ip, etc)

Note: Deployment -means identical pods
      service - is a loadbalancer on the top of those identical pods

SERVICE TYPES : 

(basics - loadbalancer types - internal loadbalancer and external loadbalancer 
internal loadbalancer - if there is any application running insdie aws, and if the same 
application wants to connect with another application which is also running inside the same aws, they
can talk to each other through internal loadbalancer)

(the traffic which is being received by the redis slave and redis master from the frontend server is 
called internal traffic) (example of internal traffic - database)

Internet traffic - anyone can access from across the world 
Internal traffic - anyone within a defined space can access the traffic - ex: jira can be accessed 
from only inside a specific office or through vpn

if you are planning to use a service as an internal loadbalancer or internal traffic/ internal uses 
then we will define the network type as NodePort 

kubernetes port numbers are automated - nodeport numbers range from - 30,000 to 32,000

TYPES: 
ClusterIP - if i have a database or cluster which need to access within then those are considered 
to be clusterIP 
NodePort - if the traffic needs to exposed to the outside then we would have it as NodePort  

Network testing pod creation : 
kubectl run curl --image=radial/busyboxplus:curl -i --tty

DNS AND NETWORK COMMUNICATION :
 
curl nslookup google.com
to connect to same namespace service from the default namespace testing pod
curl http://frontend 
from the test pod

to connect to another namespace service from the default namespace testing pod
nslookup <svc-name>.<namespace> 
nslookup nginx.test
or 
curl http://nginx.test

to connect to another deafult namespcae service pod from the default namespace testing pod
nslookup <ipaddress in dashes-->.default.pod.cluster.local
curl http://<ipaddress in dashes-->.default.pod.cluster.local
curl http://<fqdn>
(fqdn - Fully Qualified Domain Name)

to connect to another namespace service pod from the default namespace testing pod
nslookup <ipaddress in dashes-->.<namespacename>.pod.cluster.local
curl http://<ipaddress in dashes-->.<namespacename>.pod.cluster.local

HELM : 

installation of helm
sudo snap install helm - follow the instructions
note: apt is a package manager

helm repo search bitnami
if we want to install any tool on the kubernetes cluster we go with helm

(Package manager - means it installs all the dependencies along with the core package, also used for 
updating or upgrading or deleting (ex: apt, yum))

if we want jenkins in kubernetes we need to specify helm install jenkins

helm repo add bitnami https://charts.bitnami.com/bitnami (downloads the helm repositories)
helm repo ls
helm repo update (updates all the tools in the helm repositories)

helm search repo bitnami (shows all the tools which can be installed by helm)

helm install <deployment-name> bitnami/<toolname> (install any tool and creates a new deployment)
helm install test-nginx bitnami/nginx (installs nginx inside a newly created test-nginx deployment)
kubectl get svc (shows the test-nginx svc created)

curl https://localhost:<nodeportno.> (shows the nginx installed application from the svc)
to change the network from loadbalancer to NodePort
vi values.yaml 
inside the file in the place of loadbalancer we can give NodePort
if we want to add replicas then in the place of replica count we have to give 3
then after changing the values to install the package with a new deployment: 
helm install <deployment-name> <toolname> --values values.yaml
ex: helm install test2-nginx <bitnami/nginx> --values values.yaml

helm uninstall <deploymentname> <toolname>

(git clone https://github.com/IBM/helm101.git)































NAMESPACE:  

kubectl get namespace or kubectl get ns
example: if i have 5 worker nodes and i have diff apps to deployed on the worker nodes, i have to 
provide a capacity for all of them to run 
to group resources we have to craete namespaces
kubectl create ns insta
kubectl create ns meta 
kubectl create ns whatsapp

USES OF NAMESPACES: 
1) same resources under 1 namespace - so when we create a deployment we create them inside the 
namespaces
2) set quota to restrict the resources 
3) set roles or access restrictions - users under the namespace can only modify or view the resources 
under their namespaces 

kubectl apply -f <deployment-filename> -n <namepsacename> ()
ex: kubectl apply -f deploy -n insta 

kubectl get all -n <namepsacename> (shows all the resources under the namespace)
kubectl delete ns <namespacename> (deletes the namespace)

































































NOTES : 

devops people usually use the aws cli

when we send a message through kubectl it is received by the Api server - Api server will send message 
to scheduler - scheduler to control manager and etcd which will give the deatils of the availability 
of the worker nodes and then the container is created depending on the availability

Master which has 8gb ram /4vCPU's
it can handle 25 worker nodes of 8gb ram /4vCPU's

EXPLANATION OF KUBEADM INIT COMMAND : 

remote version is much newer falling back to stable version- 

Preflight - kubeadm will check if the node is good enough to work as master and then it will start 
downloading the containers such as API, CM, Scheduler as they run as containers

docker images command shows the apiserver, manaer and other images 

Certs - generates the certificates for the interconnectivity of api with cm and scheduler 
(simillar to password login)

Kubeconfig - configuration of the 

sudo ls /etc/kubernetes/ (check the contents of this folder)

kubelet - is the agent

control plane - 
sudo ls /etc/kubernetes/manifest/ (contains all the specs)

Container networking interface - 90 percent of people are using calico in the realtime, so right now 
we are using calico

Component which creates the containers on the worker nodes - scheduler


ENVIRONMENT - Developer, staging , production
App will be deployed in developing environment first then to the staging environment and then to the 
production

Development --> staging --> production
inside each env we have web, app and db
In kubernetes - Kubernetes --> Development (master and worker 1&2)--> staging (master and worker -1-4)
--> production (master and worker -1-10)
(it means we have 3 different clusters in all the environments combined)

cluster means -- master and worker together is called as a cluster

Akshay (DevOps) (CKA)

if we want to do get nodes : we have to go into each of the environment master and do get nodes
connect to dev -- kubectl get nodes
connect to prod --  kubectl get nodes
how to switch between environment clusters and connect to all the master nodes from one to other? 

MANIFESTS FILE :

TO SET ALIAS FOR ANY COMMAND : alias k=kubectl 
ex : k get pods is same as typing kubectl get pods

ADDING A .yaml manifests file inside the /etc/kubernetes/manifests creates a static pod inside the 
node

api server, controller scheduler and etcd all run on the kube-system namespace and also the daemon sets

KUBERNETES DOCUMENT: 

kubernetes.io/docs/home

ALIAS: 

alias k=kubectl
file for alias: 
vi .bashrc (in the root directory in the .bashrc file you can set the alias by adding the lines in the 
file and then type command <source .bashrc> and the alias is ready to use)


In any repository - 1 is the source code where java and other things will be there
and the other is a configuration file














TASKS : 

Learn about namespaces and cgroups

KUBERNETES IS VERY IMPORTANT : get deep into it

HELM : Real time indsutry usage... mostly what we do in the industry


Control Plane

container  - has three components -  compute, network, storage

AWS CLI - know about this
terraform is - API - Application programming interface -  Know about this

Heartbeat timestamp - worker nodes sends a message to the controller manager 

see chef, puppet, dynatrace

CRI - 

https://kubernetes.io/releases/

sudo ls /etc/kubernetes/
cat admin.conf (to see the config file)

SCP - scp labsuser@ipaddress/filepath /etc/kubernetes/admin.conf

Key Value store - KVS database

know what this error is 
W: An error occurred during the signature verification. The repository is not updated and the previous 
index files will be used. GPG error: https://packages.cloud.google.com/apt kubernetes-xenial 
InRelease: The following signatures couldn't be verified because the public key is not available: 
NO_PUBKEY B53DC80D13EDEF05

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

LATENCY FOR THE CONTAINERS

DATATYPE : string list integer dictionary float (know about all these)

APPLICATION CONFIGURATION - READ ABOUT IT

Workloads - very important

Fault tolerance & Elasticity




DOUBTS : 

Hi Swami, along with this trainings, what will you suggest additionally to grab good hold on K8s?

CKA


as a manager every 6 months you will be doing upgrades


installation process in real time for both master and worker nodes

so every worker node has to have same CRI or they can have different? 
ans - best practice is to have the same CRI on all the worker nodes

ERROR SOLVING : 
nodes and master not ready : 
sudo service kubelet status
sudo service kubelet restart

YAML 
Good example - https://editor.swagger.io/
Video learning - Yaml Tutorial | Learn YAML in 18 mins
Cheat sheet - https://learnxinyminutes.com/docs/yaml/

https://kubernetes.io/releases/

LEARN HELM AND HTO IN DEEP